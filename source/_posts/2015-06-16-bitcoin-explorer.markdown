---
layout: post
title: "Bitcoin Explorer"
date: 2015-06-16 21:27:03 +0800
comments: true
published: false
categories: 
---
比特币的区块解析程序有不少，包括最早的bitcoin explorer以及后面的abe、coinbase的toshi，但我们在开始比太的时候也需要自己有自己的api，在尝试bitcoin explorer再到试用abe，发现几乎很难用。占用磁盘空间超大、导入速度超慢、一旦导入过程出错往往面临重干。于是无奈自己开始干。在经历了各种折腾后终于拿出了比较满意的解决方案。

其他解决方案导入时间大约1个月、所占磁盘大小接近是bitcoind的数据文件空间 X 10.

我们的解决方案导入时间大概2~3天、所占磁盘大小大概是bitcoind的数据文件空间 X 2.

在这里大致的总结下优化的点

1. 真的要用ssd。  大家先不要吐槽。因为对于解决方案来说放着io提升小10倍而不用本身就是傻，更何况区块解析服务的解析和导入的过程都是io密集型的操作，解析是一个一个文件按字节去读取，解析完了写文件，导入也是读取解析文件再写入db。所以采用ssd可以明显的提高导入速度。这个io的提升不仅仅是体现在导入速度上，还体现在查询的速度上，并发蹭蹭涨。代码的优化也许也能提升10倍，但硬件直接提升却是最简单而有效的方法，架构师必选。别急，还有其他点。

2. 采用mysql的load data infile。load data infile可以从文本文件直接读取数据写入mysql，在这个过程中，mysql去掉了很多检查，在导入速度的提升上很明显。尤其是当表里的数据特别多时，如果采用sql去insert每次commit的时间都会越来越长。而load data infile则是一直的快。这个差别对于数据量小时没体现，当数据量巨大是，比如上百G时会体现出来挺明显的优势。当然由这个带来的问题也有2个：
	1. 多了预先生成得导入文件的任务，之前是可以在程序解析的同时做insert甚至是update delete。
	2. 生成的表内容应该是确定的数据，auto increase的主键是用不了，因为以它为外键时就不知道是什么了。

3. 先导入数据再创建index。好处有2：
	1. 在导入时不检查，大大加速了导入速度。当然，生成的导入文件内容也不要错了。
	2. 待导入完毕后再创建的索引空间占用更小，而且理论上效率也会更高那么一点点。

3. 优化表结构，
	1. 将交易中大量的script内容不存数据库，改存script在数据文件里file no、start position、end position。这3个字段都采用是int，会比script节约大量空间。（当然也有ssd给的信心，真需要读文件也不会太慢）。
	2. 尽可能使用unsign。
	3. 尽可能的使用int而不是string做主键

4. 导入过程中使用leveldb缓存key-value数据，当解析过程中需要查找之前的解析结果时可以直接通过key去查找到。leveldb是谷歌开发的高效率key-value db，bitcoin-core就是用他存储的blockchain的。我做了三部分内容的leveldb，其中有2个是临时的，1个是后续将一直使用的。

5. Sharding。因为是采用的mysql，而不是oracle等商用产品，所以我们采用的不是类似加分区，而是直接分表，非常的简单粗暴。如何分表？利用bitcoin-core的规则，按块文件分表。好处是分的交易数会比较的均匀。有了Sharding随着区块数据的不断增长，我们的结构也可以很好的承载所需要的压力。

其他考虑到但还没做的：

1. 针对一些超大地址的特殊处理。有一些比如1dice开头的地址，他们是网上在线赌博使用的地址，上面有成千上万的交易，占了全网很大的比重。如果查询这些地址肯定是对系统挺大的负担。

2. 